# ğŸš€ Azure Data Engineering Pipeline
### *End-to-End ETL Framework for Initial Load, Incremental Load & Backfill*

![Azure Architecture](a940aea0-be40-4b87-944f-79b4f92abfce.png)

---

## ğŸ§© Overview
This project demonstrates a **robust, parameterized, and reusable Azure Data Factory pipeline** designed to automate **Initial Loads**, **Incremental Loads**, and **Backfill operations**.

Itâ€™s built using **Azureâ€™s modern data stack** â€” ensuring scalability, modularity, and end-to-end data lineage.
The solution is suitable for both **batch** and **near real-time** ingestion needs.

---

## ğŸ—ï¸ Architecture Overview

### ğŸ”¹ End-to-End Flow
1. **Azure SQL DB (Source)** â†’ Data extracted using **ADF Linked Service**
2. **Bronze Layer (Raw)** â†’ Data stored in **Azure Data Lake Storage (ADLS)**
3. **Silver Layer (Transform)** â†’ Cleaned & transformed in **Azure Databricks / Spark**
4. **Gold Layer (Curated)** â†’ Modeled into **Delta Tables / Star Schema**
5. **Consumption Layer** â†’ Queried via **Azure Synapse Analytics** or **Power BI**

![ADF Architecture Flow](6aeeef52-61b4-491c-8985-253b178b54a5.png)

---

## âš™ï¸ Key Features

| Feature | Description |
|----------|--------------|
| ğŸ” **Initial Load** | Loads all historical data from source to target for the first time. |
| âš¡ **Incremental Load (CDC)** | Loads only new or modified data since last successful run. |
| ğŸ§± **Backfill Support** | Enables selective historical data reprocessing for a given date range. |
| ğŸ§  **Dynamic Pipeline** | Parameterized `ForEach` logic for multi-table ingestion. |
| ğŸš¨ **Alert Mechanism** | Web Activity triggers real-time alerts on failure. |
| ğŸ”’ **Secure Connections** | Managed identities and Key Vault integration for credentials. |
| ğŸ’¡ **Reusability** | Modular design â€” extend to new datasets with minimal setup. |

---

## ğŸ§° Tech Stack

| Category | Technology |
|-----------|-------------|
| **Orchestration** | Azure Data Factory |
| **Storage** | Azure Data Lake (ADLS Gen2) |
| **Compute** | Azure Databricks, Apache Spark |
| **Database** | Azure SQL Database |
| **Data Modeling** | Delta Lake, Star Schema |
| **Visualization** | Power BI |
| **Version Control** | GitHub |
| **Alerting** | Webhook / REST API (Teams, Slack, etc.) |

![Tech Stack](88da0547-de28-4326-8b41-4edac9ffc27f.png)

---

## ğŸ§® Pipeline Logic

### ğŸ§± Components
- **last_cdc** â€” Retrieves the latest processed timestamp (checkpoint).
- **AzureSQLToLake** â€” Executes data extraction query using the CDC filter and loads to ADLS.
- **IfIncrementalData** â€” Conditional activity to decide between Initial or Incremental logic.
- **Web Alert** â€” Sends alert on pipeline failure to notify monitoring channels.

---

## âš™ï¸ Parameterization & Dynamic Execution

Each table ingestion is fully parameterized for flexibility:

| Parameter | Description |
|------------|-------------|
| `table_name` | Table to be ingested |
| `load_type` | `initial` / `incremental` / `backfill` |
| `cdc_column` | Timestamp column used for delta detection |
| `target_path` | Folder path in ADLS |
| `last_cdc` | Last processed timestamp (from metadata table) |

ğŸ§© *This allows the same pipeline to dynamically process multiple tables in one execution.*

---

## ğŸ’» Example ADF Expression

```json
@activity('last_cdc').output.resultsets[0].rows[0].cdc
```

Used to fetch the most recent CDC value for incremental loading.

---

## ğŸ§° Setup & Deployment

### 1ï¸âƒ£ Clone Repository
```bash
git clone https://github.com/<your-username>/<repo-name>.git
cd <repo-name>
```

### 2ï¸âƒ£ Import into Azure Data Factory
- Go to **Manage â†’ ARM Template â†’ Import Template**.
- Upload the JSON files from the repo.
- Create or link the following resources:
  - Azure SQL Database (Source)
  - ADLS Gen2 (Target)
  - Databricks (for transformation)
  - Key Vault (for secrets)

### 3ï¸âƒ£ Configure Parameters
Update pipeline parameters with environment-specific values (connections, paths, etc.).

### 4ï¸âƒ£ Trigger Pipeline
You can trigger manually or via REST API:
```bash
POST https://management.azure.com/subscriptions/{sub-id}/resourceGroups/{rg}/providers/Microsoft.DataFactory/factories/{factory}/pipelines/{pipeline}/createRun?api-version=2018-06-01
```

---

## ğŸ“Š Monitoring & Alerts

- **Success**: Status logged in ADF Monitor tab.
- **Failure**: Triggers **Web Activity** â†’ sends JSON payload to your chosen alert endpoint (Teams, Slack, etc.).
- **Retries**: Configurable retry policies at activity level.

---

## ğŸ—‚ï¸ Folder Structure

```
azure-data-pipeline/
â”‚
â”œâ”€â”€ pipelines/
â”‚   â”œâ”€â”€ incremental_load.json
â”‚   â”œâ”€â”€ initial_load.json
â”‚   â”œâ”€â”€ backfill_pipeline.json
â”‚
â”œâ”€â”€ datasets/
â”‚   â”œâ”€â”€ source_sql.json
â”‚   â”œâ”€â”€ adls_target.json
â”‚
â”œâ”€â”€ linkedServices/
â”‚   â”œâ”€â”€ AzureSQLDatabase.json
â”‚   â”œâ”€â”€ AzureDataLake.json
â”‚   â”œâ”€â”€ AzureDatabricks.json
â”‚
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ transformations/
â”‚   â”‚   â”œâ”€â”€ clean_and_enrich.py
â”‚   â”‚   â”œâ”€â”€ delta_merge_logic.py
â”‚
â””â”€â”€ README.md
```

---

## ğŸ”® Future Enhancements

- Integrate **Great Expectations** for data validation.
- Add **metadata-driven configuration** for all table mappings.
- Implement **real-time ingestion** using Event Grid + Stream Analytics.
- Add **Power BI / Synapse dashboard** for operational monitoring.

---

## ğŸ† Results

âœ… Reduced manual maintenance with parameterized ingestion logic.  
âœ… Seamless switching between Initial, Incremental, and Backfill modes.  
âœ… Automated alerting and monitoring.  
âœ… Fully cloud-native and scalable design.

---

## ğŸ‘¨â€ğŸ’» Author

**Ankit Sharma**  
ğŸ’¼ *Data Engineer | Analytics & Automation Enthusiast*  
ğŸ“ *India*  
ğŸ“§ [ankitsharma@example.com](mailto:ankitsharma@example.com)  
ğŸ”— [LinkedIn](https://www.linkedin.com/in/ankit-sharma)

---

## ğŸŒŸ Repository Badges

![Azure Data Factory](https://img.shields.io/badge/Azure-Data%20Factory-blue?logo=microsoft-azure)
![Databricks](https://img.shields.io/badge/Azure-Databricks-orange?logo=databricks)
![Spark](https://img.shields.io/badge/Apache-Spark-red?logo=apache-spark)
![Python](https://img.shields.io/badge/Python-Scripting-yellow?logo=python)
![GitHub](https://img.shields.io/badge/VersionControl-GitHub-black?logo=github)
![Status](https://img.shields.io/badge/Status-Production%20Ready-success)
