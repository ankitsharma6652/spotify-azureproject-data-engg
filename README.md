# ğŸš€ Spotify Azure Data Engineering Pipeline  
### *End-to-End ETL Solution for Initial, Incremental, and Backfill Loads*




---

## ğŸ§© Overview  
This project demonstrates a **robust and automated data pipeline** built in **Azure Data Factory (ADF)** to handle **Initial Load**, **Incremental Load**, and **Backfill operations** efficiently.  

It integrates multiple Azure components â€” **SQL Database, Data Lake, Databricks, and Synapse Analytics** â€” to build a scalable and production-ready **ETL framework** that supports both **real-time and batch** processing.

---

## âš™ï¸ Key Features  

âœ… **Initial Load** â€” Loads the complete dataset from the source system into the Data Lake for the first time.  
âœ… **Incremental Load** â€” Loads only new or updated records using CDC logic and last modified timestamps.  
âœ… **Backfill Support** â€” Enables historical data loading for specific time periods when needed.  
âœ… **Dynamic ForEach Logic** â€” Parameterized looping for multiple table ingestion.  
âœ… **Error Handling & Alerts** â€” Webhook-based failure alerts (e.g., Slack, Teams, Email).  
âœ… **Modular & Scalable Design** â€” Easily extensible for additional sources and destinations.  

---

## ğŸ—ï¸ Architecture  

The pipeline follows a **multi-layer architecture** to ensure clean data flow and reusability.

### ğŸ”¹ Data Flow  
1. **Source Systems (SQL / APIs / Files)**  
   â†’ Extracted using **ADF Linked Services**.  
2. **Raw Layer (Bronze)**  
   â†’ Initial/Incremental load written to **Azure Data Lake Storage (ADLS)**.  
3. **Transform Layer (Silver)**  
   â†’ Processed using **Azure Databricks + Spark** for cleaning and enrichment.  
4. **Semantic Layer (Gold)**  
   â†’ Modeled into **Delta Tables** or **Star Schema** for reporting.  
5. **Consumption Layer**  
   â†’ Exposed to **Synapse / Power BI / Tableau** for analytics and visualization.

<img width="1408" height="736" alt="image" src="https://github.com/user-attachments/assets/f5c2898b-4e0f-4347-b1e0-c622545cf04e" />
---

## ğŸ§  Tech Stack  

| Component | Technology |
|------------|-------------|
| **Orchestration** | Azure Data Factory |
| **Storage** | Azure Data Lake |
| **Compute/Transform** | Azure Databricks, Apache Spark |
| **Database** | Azure SQL |
| **Version Control** | GitHub |
| **Visualization** | Power BI |
| **Notification** | Web Activity (REST API) |

<img width="1687" height="945" alt="image" src="https://github.com/user-attachments/assets/19db39c9-7a9e-423d-8c3d-2106ebd1f900" />


---

## ğŸ”„ Pipeline Design  

### ğŸ§± Components  
- **last_cdc** â†’ Retrieves last processed timestamp.  
- **AzureSQLToLake** â†’ Extracts data from SQL and stores it in ADLS.  
- **IfIncrementalData** â†’ Decides load type (Initial vs Incremental).  
- **Web Activity** â†’ Triggers failure alerts when errors occur.  

<img width="1040" height="447" alt="image" src="https://github.com/user-attachments/assets/8af1ef66-9e20-4b4c-a497-d3ad0d8f22f2" />


---

## ğŸ§® Parameterization  

The pipeline is fully parameterized:  
- **Table Name**  
- **Load Type** (Initial / Incremental / Backfill)  
- **CDC Column**  
- **Target Path**  
- **Last Processed Timestamp**

This allows running the same pipeline dynamically for multiple tables and load types.

---

## ğŸ§° Setup Instructions  

1. **Clone Repository**  
   ```bash
   git@github.com:ankitsharma6652/spotify-azureproject-data-engg.git
   cd spotify-azureproject-data-engg.git
   ```

2. **Deploy to Azure Data Factory**  
   - Import ARM template or create pipeline manually using JSON.  
   - Configure Linked Services (SQL, Data Lake, Databricks).  

3. **Configure Parameters**  
   - Update connection strings and dataset parameters.  

4. **Trigger Pipeline**  
   - Run via ADF UI or programmatically using REST API.  

---

## ğŸš¨ Error Handling & Alerts  

- On failure, pipeline sends alerts through **Web Activity** (e.g., Teams/Slack webhook).  
- Errors are logged with full activity run IDs for debugging.  

---

## ğŸ“ˆ Future Enhancements  

- Add **data quality checks** using Great Expectations.  
- Implement **metadata-driven framework** for table configuration.  
- Integrate **Azure Event Grid** for real-time triggering.  

---

## ğŸ‘¨â€ğŸ’» Author  

**Ankit Sharma**  
Data Engineer | Analytics Enthusiast  
ğŸ’¼ [LinkedIn](https://www.linkedin.com/in/ankit-sharma)  
ğŸ“§ [Email](mailto:ankitcoolji@gmail.com)
